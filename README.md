# UNetMamba
## ğŸ‘€Introduction

**UNetMamba** is the official PyTorch implementation of paper [UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of High-Resolution Remote Sensing Images](https://arxiv.org/abs/2408.11545). 

ğŸ‰Accepted by IEEE GRSL on 20/11/2024, and has been published online [IEEE Xplore](https://ieeexplore.ieee.org/document/10766630).

## ğŸ“‚Folder Structure

Prepare the following folders to organize this repo:
```none
UNetMamba-main
â”œâ”€â”€ UNetMamba
|   â”œâ”€â”€config 
|   â”œâ”€â”€tools
|   â”œâ”€â”€unetmamba_model
|   â”œâ”€â”€train.py
|   â”œâ”€â”€loveda_test.py
|   â”œâ”€â”€vaihingen_test.py
â”œâ”€â”€ pretrain_weights (pretrained weights of backbones)
â”œâ”€â”€ model_weights (model weights trained on ISPRS vaihingen, LoveDA, etc)
â”œâ”€â”€ fig_results (the segmentation results)
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ LoveDA
â”‚   â”‚   â”œâ”€â”€ Train
â”‚   â”‚   â”‚   â”œâ”€â”€ Urban
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images_png (original)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png (original)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png_convert (converted masks generated by tools/loveda_mask_convert.py)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png_convert_rgb (rgb format converted masks generated by tools/loveda_mask_convert.py)
â”‚   â”‚   â”‚   â”œâ”€â”€ Rural
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images_png 
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png 
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png_convert
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ masks_png_convert_rgb
â”‚   â”‚   â”œâ”€â”€ Val (the same with Train)
â”‚   â”‚   â”œâ”€â”€ Test
â”‚   â”‚   â”œâ”€â”€ train_val (merge Train and Val)
â”‚   â”œâ”€â”€ vaihingen (a total of 33 original images)
â”‚   â”‚   â”œâ”€â”€ test_images (9 original images, randomly selected)
â”‚   â”‚   â”œâ”€â”€ test_masks (9 original rgb masks)
â”‚   â”‚   â”œâ”€â”€ test_masks_eroded (9 eroded rgb masks, xxxx_noBoundary.tif)
â”‚   â”‚   â”œâ”€â”€ train_images (22 original images, randomly selected in remaining images)
â”‚   â”‚   â”œâ”€â”€ train_masks (22 original rgb masks)
â”‚   â”‚   â”œâ”€â”€ val_images (remaining 2 original images)
â”‚   â”‚   â”œâ”€â”€ val_masks (remaining 2 original rgb masks)
â”‚   â”‚   â”œâ”€â”€ val_masks_eroded (remaining 2 eroded rgb masks, xxxx_noBoundary.tif)
â”‚   â”‚   â”œâ”€â”€ train_1024 (train set at 1024*1024)
â”‚   â”‚   â”œâ”€â”€ test_1024 (test set at 1024*1024)
â”‚   â”‚   â”œâ”€â”€ val_1024 (validation set at 1024*1024)
â”‚   â”‚   â”œâ”€â”€ ...
```

## ğŸ› Install
```
conda create -n UNetMamba-main python=3.8
conda activate UNetMamba-main
pip install -r UNetMamba/requirements.txt
```
ğŸ’Tips: If you're having difficulty in installing "causal_conv1d" or "mamba_ssm", please refer to [causal_conv1d](https://github.com/Dao-AILab/causal-conv1d/releases) or [mamba_ssm](https://github.com/state-spaces/mamba/releases) to download the wheel files and then pip install them. 
For our UNetMamba, we installed both "causal_conv1d-1.2.0.post2+cu118torch2.0cxx11abiFALSE-cp38-cp38-linux_x86_64.whl" and "mamba_ssm-1.1.1+cu118torch2.0cxx11abiFALSE-cp38-cp38-linux_x86_64.whl". Moreover, UNetMamba is also compatible with the newest version of "causal_conv1d" and "mamba_ssm", please feel free to tryğŸ˜.

## ğŸ§©Pretrained Weights of Backbones

[pretrain_weights](https://pan.baidu.com/s/19TRZVfz6M9v0VYxiHB6mSA?pwd=82cj) 

## ğŸ§©Pretrained Weights of UNetMamba

[model_weights](https://pan.baidu.com/s/1wVVI1MPY_fnVSYg_5bLIlQ?pwd=mdwe) 

## ğŸ’¿Data Preprocessing

Download the datasets from the official website and split them as follows.

**1ï¸âƒ£LoveDA** ([LoveDA official](https://github.com/Junjue-Wang/LoveDA))
```
python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/Train/Rural/masks_png --output-mask-dir data/LoveDA/Train/Rural/masks_png_convert
python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/Train/Urban/masks_png --output-mask-dir data/LoveDA/Train/Urban/masks_png_convert

python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/Val/Rural/masks_png --output-mask-dir data/LoveDA/Val/Rural/masks_png_convert
python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/Val/Urban/masks_png --output-mask-dir data/LoveDA/Val/Urban/masks_png_convert

python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/train_val/Rural/masks_png --output-mask-dir data/LoveDA/train_val/Rural/masks_png_convert
python UNetMamba/tools/loveda_mask_convert.py --mask-dir data/LoveDA/train_val/Urban/masks_png --output-mask-dir data/LoveDA/train_val/Urban/masks_png_convert
```

**2ï¸âƒ£Vaihingen** ([Vaihingen official](https://www.isprs.org/education/benchmarks/UrbanSemLab/Default.aspx))

Generate the train set.
```
python UNetMamba/tools/vaihingen_patch_split.py 
--img-dir "data/vaihingen/train_images" --mask-dir "data/vaihingen/train_masks" 
--output-img-dir "data/vaihingen/train_1024/images" --output-mask-dir "data/vaihingen/train_1024/masks" 
--mode "train" --split-size 1024 --stride 1024
```
Generate the validation set. (Tip: the eroded one.)
```
python UNetMamba/tools/vaihingen_patch_split.py 
--img-dir "data/vaihingen/val_images" --mask-dir "data/vaihingen/val_masks_eroded" 
--output-img-dir "data/vaihingen/val_1024/images" --output-mask-dir "data/vaihingen/val_1024/masks"
--mode "val" --split-size 1024 --stride 1024 --eroded
```
Generate the test set. (Tip: the eroded one.)
```
python UNetMamba/tools/vaihingen_patch_split.py 
--img-dir "data/vaihingen/test_images" --mask-dir "data/vaihingen/test_masks_eroded" 
--output-img-dir "data/vaihingen/test_1024/images" --output-mask-dir "data/vaihingen/test_1024/masks"
--mode "val" --split-size 1024 --stride 1024 --eroded
```
Generate the masks_1024_rgb (RGB format ground truth labels) for visualization.
```
python UNetMamba/tools/vaihingen_patch_split.py 
--img-dir "data/vaihingen/val_images" --mask-dir "data/vaihingen/val_masks" 
--output-img-dir "data/vaihingen/val_1024/images" --output-mask-dir "data/vaihingen/val_1024/masks_rgb" 
--mode "val" --split-size 1024 --stride 1024 --gt

python UNetMamba/tools/vaihingen_patch_split.py 
--img-dir "data/vaihingen/test_images" --mask-dir "data/vaihingen/test_masks" 
--output-img-dir "data/vaihingen/test_1024/images" --output-mask-dir "data/vaihingen/test_1024/masks_rgb" 
--mode "val" --split-size 1024 --stride 1024 --gt
```

## ğŸ‹Training

"-c" means the path of the config, use different **config** to train different models in different datasets.

```
python UNetMamba/train.py -c UNetMamba/config/loveda/unetmamba.py
python UNetMamba/train.py -c UNetMamba/config/vaihingen/unetmamba.py
```

## ğŸ¯Testing

"-c" denotes the path of the config, Use different **config** to test different models in different datasets

"-o" denotes the output path 

"-t" denotes the test time augmentation (TTA), can be [None, 'lr', 'd4'], default is None, 'lr' is flip TTA, 'd4' is multiscale TTA

"--rgb" denotes whether to output masks in RGB format

**1ï¸âƒ£LoveDA** ([Online Testing](https://codalab.lisn.upsaclay.fr/competitions/421))
```
python UNetMamba/loveda_test.py -c UNetMamba/config/loveda/unetmamba.py -o fig_results/loveda/unetmamba_test
python UNetMamba/loveda_test.py -c UNetMamba/config/loveda/unetmamba.py -o fig_results/loveda/unetmamba_test -t 'd4'
python UNetMamba/loveda_test.py -c UNetMamba/config/loveda/unetmamba.py -o fig_results/loveda/unetmamba_rgb -t 'd4' --rgb --val
```

**2ï¸âƒ£Vaihingen**
```
python UNetMamba/vaihingen_test.py -c UNetMamba/config/vaihingen/unetmamba.py -o fig_results/vaihingen/unetmamba_test
python UNetMamba/vaihingen_test.py -c UNetMamba/config/vaihingen/unetmamba.py -o fig_results/vaihingen/unetmamba_test -t 'lr'
python UNetMamba/vaihingen_test.py -c UNetMamba/config/vaihingen/unetmamba.py -o fig_results/vaihingen/unetmamba_rgb --rgb
```

## ğŸ€Citation

If you find this repo useful in your research, please citingï¼š
```
@article{zhu2025unetmamba,
  title={UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of High-Resolution Remote Sensing Images},
  author={Zhu, Enze and Chen, Zhan and Wang, Dingkai and Shi, Hanru and Liu, Xiaoxuan and Wang, Lei},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  year={2025},
  volume={22},
  number={6001205},
  doi={10.1109/LGRS.2024.3505193}
}
```

## â¤Acknowledgement

- [GeoSeg](https://github.com/WangLibo1995/GeoSeg)
- [SSRS](https://github.com/sstary/SSRS)
- [mamba](https://github.com/state-spaces/mamba)
- [VMamba](https://github.com/MzeroMiko/VMamba)
- [causal-conv1d](https://github.com/Dao-AILab/causal-conv1d)
- [LoveDA](https://github.com/Junjue-Wang/LoveDA)
- [Swin-UMamba](https://github.com/JiarunLiu/Swin-UMamba)
- [CM-UNet](https://github.com/XiaoBuL/CM-UNet)

## â­Star History

<a href="https://star-history.com/#EnzeZhu2001/UNetMamba&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=EnzeZhu2001/UNetMamba&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=EnzeZhu2001/UNetMamba&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=EnzeZhu2001/UNetMamba&type=Date" />
 </picture>
</a>
